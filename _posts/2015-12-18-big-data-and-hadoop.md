---
layout: post
title: Big Data and Hadoop
tags:
- hadoop
- mapreduce
- data science
---

We have entered an age where the buzz word ‘big data’ is thrown around everywhere we go, but what does that term exactly mean? We will define big data and compare it to more traditional forms of data. We will find out why big data is important and what triggered this phenomenon. We’ll also learn how organizations are handling and analyzing big data, which big data technologies they are using, and the successful leaders who have taken advantage of big data. No matter where you are, big data has exploded and will eventually be part of your life if it isn’t already.

Big data is unavoidable, absolute, and ubiquitous. Chances are if you have a cell phone, a computer or even a television, you’ve felt the effects of big data in some way. All the biggest and best companies out there are realizing the necessity of big data and harnessing the power of big data to propel their business to unrealized heights. However, there’s a plethora of definitions and meanings for big data that begs the question: “What exactly is big data and how does it work?”  It would be obvious to think that big data just means lots and lots of data, but the truth is that is only part of the answer. Big data is bigger than that.
	
So how big is big data? There is no easy answer to this question, but the most agreed upon description is big data is big when it pushes the limitations of technology and constraints of business need (EMC, 2012). Big data can be characterized by the three V’s—volume, variety, and velocity (ExplainingComputers, 2012). In the most obvious sense, big data is big in volume. The amount of data in the world is quickly moving from gigabytes (GB) to terabytes (TB) to Zettabytes (ZB) (IBM Analytics, 2012). The aggregation of fragments of data and their relationships is becoming increasingly abundant. Variety refers to two types of data: structured and unstructured data. Structured data is essentially tabular data such as documents, financial records and personnel files (ExplainingComputers, 2012). Structured data are easy to search for and interpret, but unstructured data is the complete opposite. Unstructured data are things like photographs, text, audio and video, 3D models, simulations, and location data. This type of data is typically harder to categorize and process compared to traditional, structured data. Velocity can be described as the speed at which data is arriving at the enterprise level. Data that was once received in batches is now constantly being streamed and this constant flow of data is presenting major opportunities to find insight.
	
It is hard to attribute the big data phenomenon to any singular object or event. Big data’s origins come from an evolution of economies beginning with the agricultural economy. Growth in agriculture spurred the creation of machines which marked the start of the industrial economy. The focus of the industrial economy was in building things. As growth spiked, factories were created to fuel the demand to build, which created the data economy and that is where we are today. The idea behind the data economy is comprehension and knowledge and this has led to this explosion of big data (TiE SiliconValley, 2015).

Technological advancements in sensors and radio frequency identification (RFID), the internet, social media, and cloud technologies have further spurred the growth of big data. The world is becoming increasingly “intelligent, instrumented and interconnected (IBM Analytics, 2012).” This new world is generating tons of data. In 2005, there were 1.3 billion RFIDs on items like milk cartons and shipping containers. In 2011, there were over 30 billion RFIDs and continues to grow exponentially (IBM Analytics, 2012). As businesses become increasingly sensor and instrument enabled, huge volumes of data are generated with machine speed (IBM Analytics, 2012). For example, utility companies are reading more and more data from energy smart meters. Readings that used to be taken monthly have become possible every few minutes. The expansion of devices that are connected to the internet are increasingly prevalent leading to what is known as the “Internet of Things.” The Internet of Things refers to devices, appliances, and equipment that communicate with one another, gather data on how we live our lives, and enhance our lifestyle. Devices such as smartphones have allowed millions of people to connect and exchange information constantly. Social media services such as Twitter generates an exorbitant amount of data from its users. Twitter users generate 230 million tweets per day, which is roughly 12 TB. As opposed to the New York Stock Exchange, which only generates 1 TB of stock trading data per day (IBM Analytics, 2012). Big data has been fueled by the introduction of cloud technologies. Cloud technology allows companies to store, manage, and process data without having to own large, expensive servers. This ability to have economies of scale, extensibility, affordability, and agility have enabled businesses to partake in big data analytics. Ultimately, the rapid adoption of social and economic changes; new way of living; spontaneous, instantaneous, and constant exchange of data is what has triggered big data (EMC, 2012). 

The benefits of big data are quite overwhelming. Some use cases include agriculture, logistics, and insurance. The power to forecast bad weather and crop failures can help farmers plan for the future. An example of this is a company called Edyn which makes devices that are put into the soil to monitor microclimate information in real time. Freight companies have utilized big data already. For example, a company called Weft puts microchips on crates that give information about location and temperature that helps with placement. Insurance companies can connect to their customers via smartwatch app to monitor their fitness activity. Customers who exercised regularly were rewarded with discounts (TiE SiliconValley, 2015). It’s also possible big data can even help governments fight civil unrest and pandemics. The McKinsey Global Institute report estimates that big data can help the United States save $300 billion in healthcare efficiency costs. It also estimates that Europe can save $149 billion in government administration costs (ExplainingComputers, 2012). 

The importance of big data cannot be overstated. Collecting, analyzing, and understanding big data is becoming a differentiating strategy today, and will become a fact of life tomorrow (EMC, 2012).  It is imperative to get on board now and keep up with the changing world or simply get left behind. No industry is immune to big data and many organizations are already beginning to find creative ways to take advantage of big data. However, for many organizations starting out the question becomes: how do we handle analyzing big data?

Back when data was only at human scale, having large, expensive machines was suitable for computations. However, the increase of largely distributed data requires large scale distributed systems which cannot be met by traditional computing architecture. Archaic computing systems cannot handle big data processing and results in many bottlenecks. Big data analysis requires more exploratory, iterative, and comprehensive processing. One tool for data analysis is called relational database management systems (RDBMS). RDBMS is a database of tables with rows and columns and uses SQL to retrieve data. This method is extremely popular and is widely used today; however, RDBMS cannot scale up to meet the demands of big data (EMC, 2012). 

Companies have often touted the greatness of RDBMS simply because it is user friendly, but it only works well for small, structured data. For companies dealing with small amounts of data, RDBMS continues to be the preferred tool for data querying and analysis. For the rest of the world dealing with big data, which requires more complex analysis, new technology is needed.

With the vast amounts of data out there many organizations are ill equipped to analyze and procure information. As a result, most data is ignored (ExplainingComputers, 2012). The advent of big data enables organizations to extract sentiment data where this was previously uncaptured. The ability to monetize relationships from social networks gives organizations a much more comprehensive view of their customers. However, traditional, powerful machines do not have the ability to capture important insights from sentiment data. 

While traditional computing architecture was built with top of the line hardware to compute intensive applications they cannot handle today’s big data processing. These machines are always online and use a combination of cheap (SATA) and expensive (SSD) hard disks for storage. The storage area network (SAN) bridged the server and storage. Together this infrastructure created a very robust and fault tolerant system. High cost and inefficiencies made the traditional systems unfavorable in big data processing. It is estimated that 25% of the CPU is spent on sorting (EMC, 2012). The need for volume and speed at a low cost led to increasing popularity of the cluster architecture.
A cluster architecture is made up of many racks that contain nodes, which are composed of cores, memory, and disks that are connected via high speed network (EMC, 2012). There are several advantages to cluster architecture. One advantage is the racks are highly modular and scalable, which means they can be added to increase capacity. The Holy Grail advantage of cluster architecture is data locality and parallelization which are required in handling big data. Data locality refers to data being processed in the same node or rack, which eliminates network bottlenecks. Parallelization refers to processing data across many nodes simultaneously. Each node sorts pieces of data that are collocated in the node and the node transfers the sorted data from local disk to main memory allowing for quick computation (EMC, 2012). Clusters can maintain the same input/output per second (IOPS) as SSDs but at SATA price points. This is what allows clusters to be cost effective. While cluster architecture appears to be the best choice in processing, it has disadvantages as well. Commodity hardware can fail often so management of failures add complexity. By default, clusters will replicate data three times to combat risk of hardware failure, increasing the need for storage. Despite the disadvantages, clusters are the architecture of choice to handle big data.

One of the first companies to take full advantage of the cluster architecture is Google. The internet search giant determined that traditional computing architecture was too costly for the job they were trying to do. Google adopted the cluster architecture that would allow them the best performance for cost. As a result, Google was able to create the Google File System (GFS) and a simple programming model called MapReduce that could be applied to large scale computing while hiding the complexity from its users (Google Israel, 2008). 

The most prominent technology to take advantage of the cluster architecture is Hadoop. Hadoop is an open source implementation of GFS and MapReduce. It was created by Doug Cutting who named it after his son’s elephant toy. Hadoop is essentially a framework for storage and processing of big data. At the basic level, Hadoop distributes and processes large data sets across clusters of server computers. The structure of Hadoop is broken down into two components: Hadoop Distributed File System (HDFS) and MapReduce. 

HDFS primarily deals with storage. Its main function is to split, scatter, replicate and manage data across nodes. A file in the HDFS consists of equal size file blocks which are 64mb and storage blocks which are 512kb. In order to combat risks of hardware failure, HDFS replicates the file block three times on multiple nodes, which could be in different racks. All nodes are data nodes that can store data, but at least one node will be the name node. The name node decides where each file block will reside and monitors them. When a node fails, the name node identifies all file blocks affected and retrieves copies from healthy nodes and stores it in a new node. The name node will then update this information in its table. When a file needs to be read, an application connects to the name node and gets the address for disk blocks where file block is residing. The application will then be able to read blocks directly without going to the name node every time. Since the name node can be a single point of failure leading to the loss of files and their addresses, a backup node is usually appointed and contains backup copies of tables from the name node (EMC, 2012). 

MapReduce is the heart of Hadoop that enables processing data in parallel across many nodes at the same time. MapReduce is a programming model based on the java language. There are three stages to MapReduce: map, shuffle, and reduce. In MapReduce, the user just writes a program that has a map and a reduce function and Hadoop will take care of the details like file input and output, networking, synchronization, and failure recovery. MapReduce is rooted in the principle of data locality so data is processed where the data resides. Much of the computation happens on nodes with data on local disks which minimizes network traffic. When the computation is finished we can read the result back from the cluster (MapRAcademy, 2013).

The processing of data in MapReduce is composed of map and reduce jobs which are performed sequentially. In the map phase, nodes are assigned map jobs which takes part of a data set closest to it, breaks it down and generates key value pairs. The key value pairs are sent to designated nodes known as reduce nodes. During the reduce phase, the key value pairs produced from the map phase are sorted (grouped with same keys) and shuffled (sorted into one key). After sort and shuffle, the key value pairs from reduce nodes will be reduced even further into a unique key value pair as the output. 

What happens when jobs fail? Well, MapReduce has a solution to that. When a computation begins, MapReduce assigns a job tracker and multiple task trackers. A node assumes the role of job tracker like how a node assumes the role of name node in HDFS. A job tracker monitors status, orchestrates data flow, and handles failures. The task tracker keeps track of all tasks running on its nodes. Jobs are divided into tasks and are scheduled to run on other nodes by the job tracker. The job tracker will frequently ping task trackers for progress reports. If there is no response from a task tracker because a job has stalled or is taking too long, the job tracker will reschedule it on a different task tracker (EMC, 2012). Together, the job tracker and task tracker(s) help to ensure computations are completed where issues may occur when processing data in Hadoop. 

Some other big data technologies that are used today include Hbase, Hive, Pig and NoSQL databases. Hbase is a storage layer that sits on top of HDFS that allows high bandwidth storage of abstract data. Hive is based on the SQL query language that is built on top of Hadoop. Hive allows anyone to talk to a Hadoop cluster and perform Map Reduce jobs in parallel (Scolbe, 2010). Pig is a language that is used for data analysis and processing data much like Hive. However, Pig allows user defined functions which can call MapReduce jobs in other programming languages. NoSQL databases are becoming increasingly popular because of big data. Because there is so much unstructured data that is difficult to categorize or place in tabular form, NoSQL databases are the current solution to storing data such as graphs and documents. As big data presents challenges in storage and processing, Hbase, Hive, Pig, and NoSQL databases are extending the capabilities and pushing the boundaries of what is possible.

We are amidst the Analytics 3.0 era. Big data has pervaded every industry in every form. Firms that once ignored big data can no longer afford to do so. Competing on analytics is a necessary endeavor for not only large organizations, but small organizations as well. There are several successful leaders of big data some of them being Google, Facebook, Cloudera, and Twitter. Google can be described as one of the pioneers of big data. Google’s goal of indexing the entire internet is as big as big data problems come. In order to tackle this problem, Google created GFS and MapReduce as mentioned earlier. This allowed them to store and process data at incredible volume and speed. Facebook and Twitter both embody the pinnacle of social media. Both companies have amassed millions of users that generate millions of content daily. Their users have generated sentiment data that can help monetize relationships. Cloudera offers many big data solutions and training for its clients. Cloudera continues to aid in the push of big data solutions for many companies. Walmart has pioneered a strong supply chain using big data. Their implementation of analytics with vendors has allowed Walmart to become one of the leading retailers in the world. Hot new startups like Uber and Airbnb are creating businesses out of brokering big data information. These two companies are unique in that they do not own any products, but they broker the cars and the homes that is only made possible by big data.

The triggers to the big data phenomenon from sensors, internet, cloud technologies, and social media bring big opportunities as well as big challenges. In the market today, Analytics 3.0 and big data has become a major differentiator in business. Organizations continue to manage their big data needs with technologies like Hadoop, Hive, and NoSQL databases. We see this in companies such as Google, Facebook, Twitter, and Cloudera.  These businesses are leading the charge in this big data world we now live in. Big data is ultimately a vague term, but whose meaning is monumental to the new data economy.


 
<h3>References</h3>

ExplainingComputers. (2012, June 16). Explaining Big Data [Video file]. Retrieved from https://www.youtube.com/watch?v=7D1CQ_LOizA

IBM Analytics. (2012, April 3). What is IBM Big Data? Part 1 [Video file]. Retrieved from https://www.youtube.com/watch?v=B27SpLOOhWw

IBM Analytics. (2012, April 10). What is IBM Big Data? Part 2 [Video file]. Retrieved from https://www.youtube.com/watch?v=W2Vnke8ryco

EMC. (2012, February 27). Big Ideas: How Big is Big Data? [Video file]. Retrieved from https://www.youtube.com/watch?v=eEpxN0htRKI

EMC. (2012, July 17). Big Ideas: Simplifying Cluster Architectures [Video file]. Retrieved from https://www.youtube.com/watch?v=4M3cROio9vU

EMC. (2012, April 6). Big Ideas: Demystifying Hadoop [Video file]. Retrieved from https://www.youtube.com/watch?v=XtLXPLb6EXs

Google Israel. (2008, November 26). MapReduce [Video file]. Retrieved from https://www.youtube.com/watch?v=zVSSsJ_ua4Q

MapRAcademy. (2013, March 1). Intro to MapReduce [Video file]. Retrieved from https://www.youtube.com/watch?v=HFplUBeBhcM

Scolbe, Robert. (2010, March 4). What is Hadoop? Other big data terms like MapReduce? Cloudera’s CEO talks us through big data trends [Video file]. Retrieved from https://www.youtube.com/watch?v=S9xnYBVqLws

TiE SiliconValley. (2015, June 23). Big Data’s Big Impact The New Economy – TiEcon 2015 [Video file]. Retrieved from https://www.youtube.com/watch?v=n4I_NTA9lkE&feature=youtu.be
